top_level_prob = with(cohort_extra, data.frame(cohort))
View(top_level_prob)
predict(logit.overall, top_level_prob, type = "response")
summary(logit.overall)
Low.Calorie = rnorm(200, 10, 1) #Randomly generating weight loss measurements
Low.Carb = rnorm(200, 8.5, 1)   #for various diet types.
Low.Fat = rnorm(200, 8, 1)
Control = rnorm(200, 0, 1)
Weight.Loss = c(Low.Calorie, Low.Carb, Low.Fat, Control) #Combining data into
Category = c(rep("Low Calorie", 200),                    #different consolidated
rep("Low Carb", 200),                       #vectors.
rep("Low Fat", 200),
rep("Control", 200))
boxplot(Weight.Loss ~ Category,
col = c("red", "orange", "yellow", "green"),
main = "Distribution of Weight Loss\nfor Various Diets")
summary(aov(Weight.Loss ~ Category)) #Conducting the One-Way ANOVA on the weight
summary(logit.overall)
summary(aov(Weight.Loss ~ Category)) #Conducting the One-Way ANOVA on the weight
summary(aov(cohort_extra$extra ~ cohort_extra$cohort))
cohort_extra$extra <- as.numeric(cohort_extra$extra)
summary(aov(cohort_extra$extra ~ cohort_extra$cohort))
summary(logit.overall)
shiny::runApp('Downloads/NYCDSA/Project/Shiny/shiny_jackyip')
install.packages('tree')
library(tree)
library(ISLR)
help(Carseats)
attach(Carseats) # allows for column selection without calling the $ function
hist(Sales)
summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
View(Carseats)
library(tree)
library(ISLR)
help(Carseats)
attach(Carseats) # allows for column selection without calling the $ function
hist(Sales)
summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(high ~ . - Sales, split = 'gini', data = Carseats)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(high ~ . - Sales, split = 'gini', data = Carseats)
tree.carseats = tree(High ~ . - Sales, split = 'gini', data = Carseats)
summary(tree.carseats)
tree.carseats = tree(High ~ - Sales, split = 'gini', data = Carseats)
summary(tree.carseats)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
install.packages('tree')
install.packages("tree")
library(tree)
library(ISLR)
help(Carseats)
attach(Carseats) # allows for column selection without calling the $ function
hist(Sales)
summary(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
rm(list=ls())
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
tree.carseats
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10)
Carseats.test = Carseats[=train, ]
Carseats.test = Carseats[-train, ]
High.test = High[-train]
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
tree.pred = predict(tree.carseats, Carseats.test, type = 'class')
tree.pred
table(tree.pred, High.test)
(60 + 42)/120
set.seed(0)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.carseats$k, cv.carseats$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
par(mfrow = c(1, 1))
prune.carseats = prune.misclass(tree.carseats, best = 4)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred = predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
(53 + 33)/120
library(MASS)
help(Boston)
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
set.seed(0)
cv.boston = cv.tree(tree.boston)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
prune.boston = prune.tree(tree.boston, best = 4)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
summary(cv.boston)
cv.carseats
cv.boston
prune.boston = prune.tree(tree.boston, best = 7)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
plot(tree.boston)
text(tree.boston, pretty = 0)
set.seed(0)
cv.boston = cv.tree(tree.boston)
cv.boston
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
prune.boston = prune.tree(tree.boston, best = 7)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
library(randomForest)
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
set.seed(0)
oob.err = numeric(13)
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
par(mfrow = c(1, 1))
summary(boost.boston)
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
dim(predmat)
View(predmat)
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(h = min(oob.err), col = "red")
set.seed(0)
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.1)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
attach(OJ)
rm(list=ls())
load(OJ)
datga(OJ)
data(OJ)
View(OJ)
help(OJ)
set.seed(0)
train = sample(1:nrow(OJ), 0.2*nrow(OJ))
OJ.test = OJ[-train, ]
train = sample(1:nrow(OJ), 0.8*nrow(OJ))
OJ.test = OJ[-train, ]
help(OJ)
OJ.train = OJ[train, ]
View(OJ.train)
tree.train = tree(Purchase ~ ., split = 'Gini', data = OJ)
tree.OJ = tree(Purchase ~ ., split = 'gini', data = OJ)
tree.OJ
summary(tree.OJ)
OJ.pred = predict(tree.OJ, OJ.test, type = 'class')
OJ.pred
summary(OJ.pred)
Purchase.test = Purchase[-train]
table(OJ.pred, Purchase.test)
(125+61)/214
set.seed(0)
cv.OJ = cv.tree(tree.OJ, FUN = prune.misclass)
names(cv.carseats)
names(cv.OJ)
cv.OJ
plot(cv.carseats$size, cv.carseats$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
par(mfrow = c(1, 2))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$size, cv.OJ$dev, type = "a",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$size, cv.OJ$dev, type = "c",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$k, cv.OJ$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
prune.OJ = prune.misclass(tree.OJ, best = 18)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
summary(prune.OJ)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
par(mfrow = c(1, 1))
plot(prune.OJ)
text(prune.OJ, pretty = 0)
tree.pred = predict(prune.OJ, test.OJ, type = 'class')
tree.pred = predict(prune.OJ, OJ.test, type = 'class')
table(tree.pred, Purchase.test)
(127+54)/214
(125+61)/214
(127+54)/214
OJ.pred = predict(tree.OJ, OJ.test, type = 'class')
table(OJ.pred, Purchase.test)
(125+61)/214
set.seed(0)
cv.OJ = cv.tree(tree.OJ, FUN = prune.misclass)
names(cv.OJ)
cv.OJ
par(mfrow = c(1, 2))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$k, cv.OJ$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
prune.OJ = prune.misclass(tree.OJ, best = 11)
summary(prune.OJ)
prune.OJ = prune.misclass(tree.OJ, best = 21)
summary(prune.OJ)
par(mfrow = c(1, 1))
plot(prune.OJ)
text(prune.OJ, pretty = 0)
tree.pred = predict(prune.OJ, OJ.test, type = 'class')
table(tree.pred, Purchase.test)
(127+54)/214
(126+60)/214
table(OJ.pred, Purchase.test)
set.seed(0)
rf.OJ = randomForest(Purchase ~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ
rf.OJ
rf.OJ
rf.OJ
predict(rf.OJ, OJ.test, type = 'class')
x = predict(rf.OJ, OJ.test, type = 'class')
table(x, Purchase.test)
Purchase.train = Purchase[train]
x == Purchase.train
sum(x == Purchase.train)
sum(x == Purchase.test)
sum(x == Purchase.test)/length(OJ.test)
sum(x == Purchase.test)/nrow(OJ.test)
rf.OJ
help(predict)
rf.OJ
1- .2126
rf.pred = predict(rf.OJ, OJ.test, type = 'class')
rf.pred
table(rf.pred, Purchase.test)
sum(x == Purchase.test)/nrow(OJ.test)
sum(x == Purchase.test)
(114+61)
(114+61)/214
sum(rf.OJ == Purchase.test)
rf.OJ
sum(rf.pred == Purchase.test)
(114+61)/214
importance(rf.pred)
importance(rf.OJ)
varImpPlot(rf.OJ)
set.seed(0)
fit$err.rate[500, 1]
?fit
data(Boston)
View(Boston)
rm(Boston)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
View(OJ)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$err.rate[500, 1]
cat("We're performing iteration", mtry, "\n")
}
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
oob.err
min(oob.err)
1- min(oob.err)
set.seed(0)
bag.OJ = randomForest(Purchase ~ ., data=OJ , subset=train, mtry=3, importance =TRUE)
bag.OJ
yhat.bag = predict(bag.OJ, newdata = OJ[-train, ])
mean((yhat.bag - OJ.test)^2)
plot(yhat.bag , OJ.test)
plot(yhat.bag , Purchase.test)
mean((yhat.bag - Purchase.test)^2)
mean((yhat.bag - Purchase.test)^2)
table(yhat.bag, Purchase.test)
(117+59)/214
table(tree.pred, Purchase.test)
OJ.train.indicator = OJ.train
OJ.test.indicator = OJ.test
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase, mode = "numeric") - 1
OJ.test.indicator$Purchase = as.vector(OJ.test$Purchase, mode = "numeric") - 1
View(OJ.test.indicator)
set.seed(0)
?gbm
boost.OJ = gbm(Purchase ~ ., data = OJ[train, ],
distribution = "bernoulli",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001)
boost.OJ = gbm(Purchase ~ ., data = OJ.train.indicator,
distribution = "bernoulli",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.001)
par(mfrow = c(1, 1))
summary(boost.OJ)
View(OJ.train.indicator)
predmat = predict(boost.OJ, newdata = OJ[-train, ], n.trees = n.trees, tyoe = 'response')
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.OJ, newdata = OJ[-train, ], n.trees = n.trees, tyoe = 'response')
par(mfrow = c(1, 1))
berr = with(OJ[-train, ], apply((predmat - Purchase)^2, 2, mean))
View(OJ.test)
View(OJ.test.indicator)
head(predmat)
predmat = predict(boost.OJ, newdata = OJ[-train, ], n.trees = n.trees, type = 'response')
?predict
round(predmat,0)
predmat = round(predmat,0)
colSums(predmat)
predmat = colSums(predmat)
min(predmat)
max(predmat)
plot(n.trees, predmat, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
plot(n.trees, predmat, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Accuracy")
abline(h = min(oob.err), col = "red")
abline(h = max(predmat), col = "red")
abline(h = max(oob.err), col = "red")
plot(n.trees, predmat, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Accuracy")
abline(h = max(oob.err), col = "red")
abline(h = max(predmat), col = "red")
predmat = predict(boost.OJ, newdata = OJ[-train, ], n.trees = n.trees, type = 'response')
View(predmat)
predmat = colSums(predmat)
predmat
predmat = round(predmat,0)
min(predmat)
predmat
plot(n.trees, predmat, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Accuracy")
predmat = predict(boost.OJ, newdata = OJ[-train, ], n.trees = n.trees, type = 'response')
predmat = colSums(predmat)
predmat = round(predmat,1)
min(predmat)
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.OJ, newdata = OJ[-train, ], n.trees = n.trees, type = 'response')
predmat = colSums(predmat)
predmat = round(predmat,2)
min(predmat)
predmat
plot(n.trees, predmat, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(h = min(predmat), col = "red")
abline(h = min(oob.err), col = "red")
tree.pred
prune.OJ
summary(prune.OJ)
cv.OJ
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$k, cv.OJ$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
prune.OJ = prune.misclass(tree.OJ, best = 21)
d
d
plot(n.trees, predmat, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(h = min(predmat), col = "red")
abline(h = min(oob.err), col = "red")
min(oob.err)
predmat = predict(boost.OJ, newdata = OJ[-train, ], n.trees = n.trees, type = 'response')
predmat = colSums(predmat)
min(predmat)
predmat = round(predmat,2)
min(predmat)
predmat
load("~/Downloads/NYCDSA/Project/Kaggle/randomforest.RData")
View(rf.train_imputed)
head(train$public_healthcare_km)
View(head(train$public_healthcare_km,200)
View(head(train$public_healthcare_km,200)
View(head(train$public_healthcare_km,200))
View((train$public_healthcare_km,200)
summary(train$public_healthcare_km)
setwd("~/Downloads/NYCDSA/Project/Kaggle/github")
cleaning <- read.csv('full_data_kaggle.csv')
View(cleaning)
?read.csv
cleaning <- cleaning[,-1]
View(cleaning)
plot(full_sq)
plot(cleaning$full_sq)
View(cleaning)
View(rf.train_imputed)
